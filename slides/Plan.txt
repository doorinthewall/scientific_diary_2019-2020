Здравствуйте...

Сверточные нейросети оказались очень мощным инструментом для работы с табличными данными, изображениями, текстом, звуком. Но как обобщить этот подход на данные представленные графами, например молекулы, человеческие тела и т.д. Одно из решений это графовые нейронные сети. 

Такие сети бывают нескольких видов, существуют даже графовые автокодировщики. Но я расскажу о сверточных сетях и рассмотрю несколько статей посвященных им. 

Сверточные сети бывают спектральные, основанные на спектральном разложении лапласиана графа и пространственные(spatial). Лапласианом называют матрицу следующего вида:...пояснение что есть что... В спектральной теории графов большее значение играет спектральное разложение Лапласиана. Например имеет место быть графовое разложение фурье: ...пояснить что есть что в векторном виде и в виде суммы... Интересной аналогией с разложением фурье является то что прибольших собственных значениях соответствующие собственные векторы чаще пересекают ноль(стр4, https://arxiv.org/pdf/1211.0053.pdf), можно сказать что функции из базиса осцеллируют чаще при больших лямбда как и в разложении фурье при больших значениях кси функции имеют большую частоту.

Новое пространство полученное в результате преобразования фурье называют спектральным. И светртка в изначальном пространстве будет выглядеть как произведение в спектральном (Теорема о свертке). Соответственоо свертка будет выглядеть следующим образом: формула...
Но поиск собственных чисел достаточно тяжеловесная операция поэтому используются полиномы чебышева, для аппроксимации спектрального фильтра. В статье Кипфа и Веллинга предлагается использовать апроксимацию первого порядка, и свертка в конечном итоге принмает такой вид. Здесь заложены две особенности: количество прпаметров тета уменьшено, для избежания переобучения, т.к собственные значения нормированного лапласиана лежат на отрезеке от 0 до 2х, и они по сути являются коэффициентами полинома, для избежания взрывов и затуханий градиента используется  renormalization trick. 

В последствии такая свертка стала довольно популярной, и можно сказать что это обоснование пространственных сверток. лямбда выбрана в качестве двойки потому что для нормированного Лаплассиана возможные значения собственных значений лежат на отрезке от 0 до 2х и равны двум только в случе двупольного графа, предполагается что сеть сама сможет адаптировать выходы к этой константе. К сожалению проблема заключается в невозможности масштабирования этой процедуры, по количеству ребер она линейна и вычисляется по всему датасету.



Но есть техники сэмплирования вершин из графа, позволяющие обойти это огрничение.(если успею все остальное прочитать про GraphSAGE про сэмплирование) 

Теперь от теории к практике...

На конференции CVPR 2019 вышли две статьи о применении графовых сверточных сетей для задач мультилэйблинга изображений и распознавания и предсказания действий человека соответственно. 

В первой авторы предлагают достаточно интересный подход: вершины графа это лэйблы, ребра это соответственно признак того встречаются ли данные лэйблы на одном и том же изображении в выборке или нет. В отличии от Кипфа и Веллинга в этом подходе графовая сеть на выходе предсказывает векторы классификаторов для каждого лэйбла, которые применются к выходам Res-Net-101, используемой для извлечения признаков. Еще одной интересной особенностью является то что изначально матрица представления вершин состоит из эмбедингов слов которыми описаны лэйблы. Матрица смежности графа формируется согласно следующему правилу, оснавнному на условных вероятностях встречаемости лэйблов, оцененных по датасету: формула. 

Таким образом опреция графовой свертки это грубо говоря взвешанная сумма признаков самой вершины и ее соседей. Но бинарная матрица корреляций приводит к эффекту сглаживания(oversmoothing), когда образующиеся в результате свертки признаки вершин, относящихся к разным кластерам, становятся неразличимы. Преодолевают авторы этот эффект с помощью перевзвешивания. Только вот мне кажется что в статье оно указано не совсем правильно и перевзвешивать надо именно только коррелированных соседей а не весь граф. У этой статьи есть реализация на гитхабе от авторов, и в коде выглядит так.

def gen_A(num_classes, t, adj_file):
    import pickle
    result = pickle.load(open(adj_file, 'rb'))
    _adj = result['adj']
    _nums = result['nums']
    _nums = _nums[:, np.newaxis]
    _adj = _adj / _nums
    _adj[_adj < t] = 0
    _adj[_adj >= t] = 1
    _adj = _adj * 0.25 / (_adj.sum(0, keepdims=True) + 1e-6)
    _adj = _adj + np.identity(num_classes, np.int)
    return _adj

def gen_adj(A):
    D = torch.pow(A.sum(1).float(), -0.5)
    D = torch.diag(D)
    adj = torch.matmul(torch.matmul(A, D).t(), D)
    return adj


Следующая статья о предсказании движений. В ней авторы на основе скелетных представлений тел пытаются решить две основные задачи: предсказание действий и распознаваний. Авторы используют два вида связей между точками: структурные, которые явно извлекаются из данных, и связи на основе действий, которые извекаются из данных с помощью графовых сетей. Собственно за эти свзи отвечают разные части архитектуры описанной в статье и обучаются они по отдельности соответственно на вышеупомянутых задачах, но бэкбоун общий. 

AIM часть архитектуры, для нахождения связей в контексте действий. Этот модуль представляет из себя кодирровщик декодеровщик архитектуру. Кодировщик принимает на вход массив кооридинат суставов во времени и выдает тензор из нулей и единиц, каждый элемент которого это веороятность связи между конкретными двумя суставами в контексте конкретного вида действия.

Для этого кодировщик итеративно уточняет признаки связей с помошью процедуры указаной на слайде где + -- операция конкатенации. f- -- нейросети, F--функция аггрегации, например усреднение или поэлементная максимизация. Затем вероятности вычисляются с помошью гумбель софтмакса. 

Декодировщик предсказывает будущие положения координат с помощью следующей процедуры: формулы

Обучение прооисходит с помощью функции потерь которая выглядит так:

Где A0 наши априорные представления о вероятностях действий. Т.к существуют варианты, особенно если мы не хотим рассматривать большое количество действий а значит и большое C, когда две части тела вообще никогда не взаимодействуют. Поэтому добавляется призрачная связь, которая означает что части тела не взаимодействуют. В таком случае априорная матрица выглядит следующим образом.

В конечном итоге авторы использут матрицу A в свойе свертке в контексте действий.

Структурные связи авторы разбивают на три множества: сама точка, точки которые находятся ближе к центру масс, остальные. Здесь элементы матрицы играют роль вероятностей. матрицы W и M отвечают за важности признаков и ребер соответственно. 

Итоговым результатом статьи становится ASGC свертка, которая является суммой вышеупомянутых сверток и AS-GCN блок. Соответственно вся конструкция обучается на задаче распознавания действия: на выход сети применяется аверэйдж пулинг по последним двум размерностям, к полученному вектору применяется софтмакс классификатор.  В случе предсказания позы строится дополнительная голова  и она уже учится с помощюью mse. Результаты... 