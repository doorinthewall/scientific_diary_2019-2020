Здравствуйте, сегодня я хочу рассказать вам о мета learning --- перспективном направлении в машинном обуении, и о задачах для которых был успешно применен данный подход. Идея заключается в построении системы, которая поможет улучшенить работу модели для конкретной задачи или адаптировать ее для решения новых задач. Аналогия "--- эволюционный процесс, создавший человеческий мозг. 

Первая задача - настройка черного ящика. Нейронные сети хорошо показали себя в  обработке изображений и речи, построении языковых моделей, но конструирование нейросети остается непростой задачей, требующей определенных навыков и времени. Что будет, если доверить эту работу другой нейросети!

Так и поступили авторы статьи. Они использовали следующие ключевые идеи: архитектура нейросети может быть записана как строка произвольной длины, можно обучить RNN, назовем ее controller, для поиска архитектуры, каждый выход  интерпретировать как определенный параметр искомой архитектуры(Количество фильтров в слое сверточной нейросети, с какими слоями соединен данный слой(skip connections) и т.д.). Обучить контроллер можно с помощью reinforcement-learning, считая что вознаграждение это точность полученной архитектуры на кросс-валидации.

На данном слайде изображена архитектура контроллера для сверточной нейросети, цветные прямоугольники --- выходы блоков контроллера. Они представляют собой выходы софт-макс классификатора. Блоки controller'а сгруппированы по слоям искомой архитектуры. 

Если искомая архитектура --- рекурентная нейросеть, то вместо параметров сверточных блоков, контроллер предсказывает операции над входами будущей архитектуры.

Теперь о процессе обучения controller. Концепция обучения с подкреплением состоит в том, что мы обучаем агента через взаимодействие со средой. В каждый момент времени агент находится в определенном средой состоянии, предпринимает в соответствии со своей стратегией действие и переходит в новое состояние. Это новое состояние опять определяет среда. Агент за каждое свое действие получает вознагараждение, оно показывает насколько действие удачно. Стратегия может быть детерменирована, т.е каждому состоянию строго определяется действие. Или она может быть стохастической, т.е играть роль распределения над действиями агента. В нашем случае стратегия стохастическая. В такой постановке задача сводится к оптимизации следующего функционала, грубо говоря мы ищем такую стратегию, которая позволяет получить наибольшую награду. 

Как его максимизировать? Существует множество способов. Авторы статьи используют policy gradient. Предлагается оптимизировать  стратегию напрямую. Для этого необходимо оценить градиент функционала. Что такое мат ожидание награды по стратегии? Награда определяется траекторией агента. Траектория это цепочка действий и состояний во времени. Вероятность появления любой такой цепочки определяется стратегией агента и динамикой среды. Динамика среды определяет новое состояние агента. Тогда градиент функционала можно преобразовать таким образом. В результате знак градиента уходит под знак мат ожидания. Дать несмещенную оценку мат ожидания новой случайной величины можно с помощью среднего наблюдений.

Проблема заключается в том, что такая оценка может иметь сильный разброс, она зависит от конретных траекторий, которые мы пронаблюдали. Вот пример который иллюстрирует эту проблему... Таким образом простое решение заключается в том, чтобы отнять некий базис. Например среднее вознаграждений. Но будет ли это решением. Ответ да, оценка остается несмещенной, доказательство этого факта приведено ниже.

Таким образом окончательно алгоритм поиска идеальной архитектуры выглядит так...

Результаты для классификации сравнимы с результатами лучших, созданных человеком архитектур.

Здесь приведены результаты рекуррентных архетектур, моделирующих язык. Perplexity - мера неуверенности модели, чем меньше тем лучше алгоритм справился с задачей. Как видно у полученной контроллером нейросети она меньше всего.

На этом слайде приведена визуализация полученных архитектур. 

Следующая проблема для которой meta обучение может стать эффективным решением. Есть задачи, где обучение напрямую либо очень дорого, либо невозможно. Предлагается перенести этап обучения в симулятор. Но чем симулятор ближе к имитируемой среде, тем он сложнее, а в простом (относительно) симуляторе обучить хорошую модель невозможно. 

Но невозможно ли? Что если научить модель адаптироваться к новой среде. Тогда не обязательно учить модель в детально проработанном симуляторе. Можно взять множество простых симуляций с разными параметрами и научить агента адаптироваться к меняющейся среде. Тогда модель сама поймет как надо действовать в настоящей среде. В данной статье авторы учат робота двигать шайбу в указанное на столе место. Параметрами в таком случае будут масса конечностей робота, масса и трение шайбы, высота стола и т.д. Чтобы стратегия смогла адаптироваться к среде можно наделить ее памятью. Тогда по цепочке состояний и действий можно будет определить особенности среды. 

Для обучения рекуррентных детерминированных policy функций есть специальный алгоритм. Это вариация policy gradient, с той особенностью, что здесь вместо обычного вознаграждения используется action-value функция. Она пытается оценить возможное вознаграждение с учетом стратегии и обучается с помощью равенства беллмана.

Еще одно ноу-хау.Одна из главных проблем в RL --- исследование среды(exploration). Рандомные действия на первых шагах должны приносить награду, иначе нечего учить. В случае разреженной функции вознаграждений обучение почти невозможно. Данный метод предлагает переигрывать прошлые неудачные попытки, считая  что цель(goal) на самом деле был достигнут. 

Рассмотрим алгоритм в деталях. Сначала сэмплируем начальное состояние и настоящую цель, это может быть какой-то предикат от конечного состояния. Затем агент действует со своей стратегией какое-то время и попадает в некоторое конечное состояние. Каждый переход в новое состояние записывается в буфер. Затем мы сэмплируем дополнительные цели для текущего эпизода обучения. И добавляем их в буфер. Сэмплируем подвыборку из буффера и обучаемся на ней.

Здесь представлен конечный алгоритм, который был использован авторами статьи.

Следующий тип задач -- задачи с небольшим количеством данных. Как известно нейросети с большим количеством параметров хорошо решают задачи с еще большим количеством данных. Но что если данных не так много. Можно ли предобученную нейросеть, например научить классифицировать чайник по пяти его изображениям. Т.е ставится задача адптации алгоритма под новые задачи. Одно из возможных решений заключается в следующем. Представим что у нас есть множество задач машинного обучения, и параметризованая модель, которая должна все эти задачи решать. Одна задача это например один класс изображений. Тогда для эффективного обучения алгоритма без переобучения необходимо найти те параметры модели, которые сильно влияют на функцию потерь каждой такой задачи, и найти такую инициализацию весов модели, что из этого состояния до оптимума любой задачи можно добраться за один-два градиентных шага. Так как мы делаем очень общие предположения, то и задачи могут решаться самые разные. От классификации до RL. 

Рассмотрим алгоритм в деталях. Сэмплируем задачи, для каждой задачи сэмплируем K наблюдений и проводим шаг градиентного спуска. Таким образом мы создаем несколько копий нашей модели, но с обновленными весами. Пересчитываем функции потерь для новых моделей и считаем градиент для обновления старых весов.

Рассмотрим пример работы MAML. Есть предобученная модель, которая хорошо моделирует синус. Требуется адаптировать ее для построения синуса другой амплитуды и периода по небольшому количеству точек. Слева показан результат для модели, предобученой с помощью MAML, справа результат обычной предобученой модели. 

Другое возможное решение данной проблемы --- создание новой модели, meta-learner'а, который будет определять параметры обучения основного алгоритма. Ключевым наблюдением стало то, что формула градиентного шага похожа на обновление памяти ячейки LSTM сети.      